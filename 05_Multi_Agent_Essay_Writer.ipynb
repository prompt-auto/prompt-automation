{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prompt-auto/prompt-automation/blob/main/05_Multi_Agent_Essay_Writer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langgraph\n",
        "# !pip install langchain-groq\n",
        "# !pip install tavily-python"
      ],
      "metadata": {
        "id": "LN_2k87lR33x"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, Annotated, List\n",
        "import operator\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ChatMessage\n",
        "from pydantic import BaseModel\n",
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "from tavily import TavilyClient"
      ],
      "metadata": {
        "id": "s_gfCSPwj258"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groq_key=userdata.get(\"GROQ_API_KEY\")\n",
        "tavily_key = userdata.get(\"TAVILY_API_KEY\")"
      ],
      "metadata": {
        "id": "dQBHmJ7qj6oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    task: str\n",
        "    lnode: str\n",
        "    plan: str\n",
        "    draft: str\n",
        "    critique: str\n",
        "    content: List[str]\n",
        "    queries: List[str]\n",
        "    revision_number: int\n",
        "    max_revisions: int\n",
        "    count: Annotated[int, operator.add]"
      ],
      "metadata": {
        "id": "COA51aMaj9un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Queries(BaseModel):\n",
        "    queries: List[str]"
      ],
      "metadata": {
        "id": "4O1C9mF7j_9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ewriter():\n",
        "    def __init__(self):\n",
        "        self.model=ChatGroq(api_key=groq_key, model_name=\"Gemma2-9b-It\")\n",
        "        self.PLAN_PROMPT = (\"You are an expert writer tasked with writing a high level outline of a short 3 paragraph essay. \"\n",
        "                            \"Write such an outline for the user provided topic. Give the three main headers of an outline of \"\n",
        "                             \"the essay along with any relevant notes or instructions for the sections. \")\n",
        "        self.WRITER_PROMPT = (\"You are an essay assistant tasked with writing excellent 3 paragraph essays. \"\n",
        "                              \"Generate the best essay possible for the user's request and the initial outline. \"\n",
        "                              \"If the user provides critique, respond with a revised version of your previous attempts. \"\n",
        "                              \"Utilize all the information below as needed: \\n\"\n",
        "                              \"------\\n\"\n",
        "                              \"{content}\")\n",
        "        self.RESEARCH_PLAN_PROMPT = (\"You are a researcher charged with providing information that can \"\n",
        "                                     \"be used when writing the following essay. Generate a list of search \"\n",
        "                                     \"queries that will gather \"\n",
        "                                     \"any relevant information. Only generate 3 queries max.\")\n",
        "        self.REFLECTION_PROMPT = (\"You are a teacher grading an 3 paragraph essay submission. \"\n",
        "                                  \"Generate critique and recommendations for the user's submission. \"\n",
        "                                  \"Provide detailed recommendations, including requests for length, depth, style, etc.\")\n",
        "        self.RESEARCH_CRITIQUE_PROMPT = (\"You are a researcher charged with providing information that can \"\n",
        "                                         \"be used when making any requested revisions (as outlined below). \"\n",
        "                                         \"Generate a list of search queries that will gather any relevant information. \"\n",
        "                                         \"Only generate 2 queries max.\")\n",
        "        self.tavily = TavilyClient(api_key=tavily_key)\n",
        "        builder = StateGraph(AgentState)\n",
        "        builder.add_node(\"planner\", self.plan_node)\n",
        "        builder.add_node(\"research_plan\", self.research_plan_node)\n",
        "        builder.add_node(\"generate\", self.generation_node)\n",
        "        builder.add_node(\"reflect\", self.reflection_node)\n",
        "        builder.add_node(\"research_critique\", self.research_critique_node)\n",
        "        builder.set_entry_point(\"planner\")\n",
        "        builder.add_conditional_edges(\n",
        "            \"generate\",\n",
        "            self.should_continue,\n",
        "            {END: END, \"reflect\": \"reflect\"}\n",
        "        )\n",
        "        builder.add_edge(\"planner\", \"research_plan\")\n",
        "        builder.add_edge(\"research_plan\", \"generate\")\n",
        "        builder.add_edge(\"reflect\", \"research_critique\")\n",
        "        builder.add_edge(\"research_critique\", \"generate\")\n",
        "        memory = MemorySaver()\n",
        "        self.graph = builder.compile(\n",
        "            checkpointer=memory,\n",
        "            # interrupt_after=['planner', 'generate', 'reflect', 'research_plan', 'research_critique']\n",
        "        )\n",
        "\n",
        "    def plan_node(self, state: AgentState):\n",
        "        messages = [\n",
        "            SystemMessage(content=self.PLAN_PROMPT),\n",
        "            HumanMessage(content=state['task'])\n",
        "        ]\n",
        "        response = self.model.invoke(messages)\n",
        "        return {\"plan\": response.content,\n",
        "               \"lnode\": \"planner\",\n",
        "                \"count\": 1,\n",
        "               }\n",
        "    def research_plan_node(self, state: AgentState):\n",
        "        queries = self.model.with_structured_output(Queries).invoke([\n",
        "            SystemMessage(content=self.RESEARCH_PLAN_PROMPT),\n",
        "            HumanMessage(content=state['task'])\n",
        "        ])\n",
        "        content = state['content'] or []  # add to content\n",
        "        for q in queries.queries:\n",
        "            response = self.tavily.search(query=q, max_results=2)\n",
        "            for r in response['results']:\n",
        "                content.append(r['content'])\n",
        "        return {\"content\": content,\n",
        "                \"queries\": queries.queries,\n",
        "               \"lnode\": \"research_plan\",\n",
        "                \"count\": 1,\n",
        "               }\n",
        "    def generation_node(self, state: AgentState):\n",
        "        content = \"\\n\\n\".join(state['content'] or [])\n",
        "        user_message = HumanMessage(\n",
        "            content=f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\")\n",
        "        messages = [\n",
        "            SystemMessage(\n",
        "                content=self.WRITER_PROMPT.format(content=content)\n",
        "            ),\n",
        "            user_message\n",
        "            ]\n",
        "        response = self.model.invoke(messages)\n",
        "        return {\n",
        "            \"draft\": response.content,\n",
        "            \"revision_number\": state.get(\"revision_number\", 1) + 1,\n",
        "            \"lnode\": \"generate\",\n",
        "            \"count\": 1,\n",
        "        }\n",
        "    def reflection_node(self, state: AgentState):\n",
        "        messages = [\n",
        "            SystemMessage(content=self.REFLECTION_PROMPT),\n",
        "            HumanMessage(content=state['draft'])\n",
        "        ]\n",
        "        response = self.model.invoke(messages)\n",
        "        return {\"critique\": response.content,\n",
        "               \"lnode\": \"reflect\",\n",
        "                \"count\": 1,\n",
        "        }\n",
        "    def research_critique_node(self, state: AgentState):\n",
        "        queries = self.model.with_structured_output(Queries).invoke([\n",
        "            SystemMessage(content=self.RESEARCH_CRITIQUE_PROMPT),\n",
        "            HumanMessage(content=state['critique'])\n",
        "        ])\n",
        "        content = state['content'] or []\n",
        "        for q in queries.queries:\n",
        "            response = self.tavily.search(query=q, max_results=2)\n",
        "            for r in response['results']:\n",
        "                content.append(r['content'])\n",
        "        return {\"content\": content,\n",
        "               \"lnode\": \"research_critique\",\n",
        "                \"count\": 1,\n",
        "        }\n",
        "    def should_continue(self, state):\n",
        "        if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
        "            return END\n",
        "        return \"reflect\""
      ],
      "metadata": {
        "id": "aZhDshGokDHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MultiAgent = ewriter()"
      ],
      "metadata": {
        "id": "m-BLJ9u2kFmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(MultiAgent.graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ],
      "metadata": {
        "id": "LCayAoNiVm3n"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic = 'pizza shop'\n",
        "initial_input = {'task': topic,\"max_revisions\": 2,\"revision_number\": 0,\n",
        "                 'lnode': \"\", 'planner': \"no plan\", 'draft': \"no draft\", 'critique': \"no critique\",\n",
        "                 'content': [\"no content\",], 'queries': \"no queries\", 'count':0}\n",
        "thread = {\"configurable\": {\"thread_id\": \"1\"}}"
      ],
      "metadata": {
        "id": "EMg9WmTTbEIX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for s in MultiAgent.graph.stream(initial_input, thread):\n",
        "    print(s)"
      ],
      "metadata": {
        "id": "0A9GpWgFkIcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EgzJnamWWphg"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}